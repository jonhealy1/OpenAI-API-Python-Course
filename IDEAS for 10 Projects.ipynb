{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82932c75",
   "metadata": {},
   "source": [
    "# 10 OpenAI API Projects\n",
    "\n",
    "**Goal of the course is to have students learn the OpenAI API use cases, probably by the 10th one they will realize its mainly just figuring out the correct prompt to give GPT-3. So as we go from #1 to #10 we'll want to add in more and more GUI functionality or make interactions look better. Feel free to reorder these however you see fit in terms of easiest to hardest, you may want to hold off on numbering the folders until you finish all of them to figure out which took the most code. We'll use the GPT API, Text Embedding API, and DALLE API, as well as translation via GPT.**\n",
    "\n",
    "Best Resources:\n",
    "* https://beta.openai.com/docs/introduction\n",
    "* https://beta.openai.com/examples\n",
    "* https://github.com/openai/openai-cookbook\n",
    "* https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a3ca5",
   "metadata": {},
   "source": [
    "## 1.) NLP Interface for DataBase/CSV\n",
    "\n",
    "**GOAL: Have a normal person just type out a question/prompt in natural language to get results from tabular database: \"What is the total sales for Q4\"? Then an answer or tabular output is shown, behind the scenes, NLP-->SQL-->DB-->Result**\n",
    "\n",
    "\n",
    "* INPUT: NLP question about a tabular dataset (CSV file for example)\n",
    "* Process:\n",
    "    * Create Database in RAM from a CSV file (can use pandas)\n",
    "        from sqlalchemy import create_engine\n",
    "        import pandas as pd\n",
    "        temp_db = create_engine('sqlite:///:memory:')\n",
    "        data = pd.read_csv('example.csv')\n",
    "        df = data.to_sql(name='data',con=temp_db)\n",
    "    * Use input() to grab NLP question\n",
    "        example = input(\"How many people have cars?\")\n",
    "    * Use OpenAI API to convert example NLP to SQL\n",
    "    * SQL queries database and returns results\n",
    "    * All of the above is behind the scenes, interface is a nice NLP input box with a formatted pandas df output.\n",
    "    \n",
    "Example code from docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"code-davinci-002\",\n",
    "  prompt=\"### Postgres SQL tables, with their properties:\\n#\\n# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n#\\n### A query to list the names of the departments which employed more than 10 employees in the last 3 months\\nSELECT\",\n",
    "  temperature=0,\n",
    "  max_tokens=150,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"#\", \";\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e1d2c",
   "metadata": {},
   "source": [
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef610d97",
   "metadata": {},
   "source": [
    "## 2.) Automatic test creator/grader\n",
    "\n",
    "**GOAL: A teacher in elementary/high school teaching on a specific topic (i.e. US History) can create an automated multiple choice quiz with an answer key by simply providing some input on the topic. Would be great if using a little python we can also automate grading of this quiz? Not sure how UI heavy this would be though?**\n",
    "\n",
    "* INPUT: NLP input of a educational topic, would be great if this can be specific, but obviously you may be limited to simple topics based on GPT knowledge.\n",
    "\n",
    "* Process: \n",
    "    * Connect with OpenAI, give it a topic\n",
    "    * OpenAI creates the list of questions with an answer\n",
    "    * Python script will probably have to manually split the answer from the question, given the way OpenAI output works.\n",
    "    * maybe look into a Python library already in existence for quiz making? (https://www.google.com/search?q=python+multiple+choice+quiz+maker)\n",
    "    * GUI matters here, need both student and teacher perspective\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai_secret_manager\n",
    "\n",
    "# Use the openai_secret_manager library to get the API key\n",
    "secrets = openai_secret_manager.get_secrets(\"openai\")\n",
    "api_key = secrets[\"api_key\"]\n",
    "\n",
    "import openai\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Define the prompt for the quiz\n",
    "prompt = (f\"Create a multiple choice quiz on the topic of US History. Each question should have 4 options.\"\n",
    "           \"Also include the correct answer for each question.\")\n",
    "\n",
    "# Make the API call to generate the quiz\n",
    "response = openai.Completion.create(engine=\"text-davinci-002\", prompt=prompt)\n",
    "\n",
    "#print the response\n",
    "print(response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4404c",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c444dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f2050b6",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3c8df",
   "metadata": {},
   "source": [
    "## 3.) Recipe Maker with DALL-E Food image\n",
    "\n",
    "**GOAL: Given a list of food items at your house, can GPT create a recipe for you, then have DALLE create an image of the dish. The next project expands on this by auto-pushing a blog post to Github pages (hosted on github.io)**\n",
    "\n",
    "* Process: \n",
    "    * Guide for blog post text gen: \n",
    "        * https://beta.openai.com/docs/guides/completion/prompt-design\n",
    "    * Guide for image gen: \n",
    "        * https://beta.openai.com/docs/guides/images/usage\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74d129",
   "metadata": {},
   "source": [
    "    \n",
    "## 4.) Automatic Blog Post creator, with image options and full blog text\n",
    "\n",
    "**GOAL: Given a prompt for a blog post and points/topics to discuss, create the text of a blog post and use DALLE to create an image for the blog post. Would be great if this exported to something like github.io website, which is free to host but we can also then just \"click a button and have a blog post appear with a git push\". We can think of this as a simple introduction to having a live site powered by OPEN AI**\n",
    "* Process: \n",
    "    * Guide for blog post text gen: \n",
    "        * https://beta.openai.com/docs/guides/completion/prompt-design\n",
    "    * Guide for image gen: \n",
    "        * https://beta.openai.com/docs/guides/images/usage\n",
    "    * Guide for github.io:\n",
    "        * https://pages.github.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87294f",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b5053",
   "metadata": {},
   "source": [
    "## 5.) Sentiment Analysis on Stock Tweets or Reddit\n",
    "\n",
    "**GOAL: Using a bit of web-scraping or an API, grab text information about tweets or reddit posts, then identify the tickers mentioned (i.e. GOOG) and then state whether the overall sentiment of the tweets of the last day/week were positive or negative**\n",
    "\n",
    "* https://beta.openai.com/examples/default-adv-tweet-classifier\n",
    "* https://beta.openai.com/examples/default-tweet-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\\n\\nTweet: \\\"I loved the new Batman movie!\\\"\\nSentiment:\",\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f5aca",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d23d",
   "metadata": {},
   "source": [
    "## 6.) Automatic Ad/Press Release Creation Tool\n",
    "\n",
    "**GOAL: Create advertisement copy and DALLE image based on a product description. Best examples would probably be pretend Apple Products, like the iPhone 20, or a new product line from Apple, like an Apple designed fridge (look at Xiaomi for inspriation of possible products, they are like the Apple of China, but create way more products). We could also use Amazon as examples, since they are famous for their Internal Press Releases before they even begin product development, this could be a bot to create those Internal Press Relases (read more on this idea below):**\n",
    "\n",
    "* Amazon Internal Press Release Ideas:\n",
    "    * https://andreamarchiotto.medium.com/amazon-press-release-how-to-55d61188ecdd\n",
    "    * https://medium.com/@DanielYubi/internal-press-release-be41de94647f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6c7c3",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60dafa",
   "metadata": {},
   "source": [
    "## 7.) Automatic Private Chat Tutor\n",
    "\n",
    "**GOAL: Self-explanatory, create a bot that can answer questions. Here would be a good time to fine-tune it. Can we have the bot read all of shakespeare to be fine-tuned on Shakespeare text, and answer questions about it?**\n",
    "\n",
    "* Process:\n",
    "    * Get Shakespeare text QA data from internet? Cliffnotes or Spark notes on Shakespeare's works?\n",
    "    * format text data correctly for fine tunning (you may need to choose a different example than Shakespeare text, as this may not teach the model about Shakespeare, but instead teach the model to speak like Shakespeare, which is not what we want!)\n",
    "    * Fine tune GPT Model\n",
    "    * Create tutor chatbot on our fine-tuned model\n",
    "\n",
    "* https://beta.openai.com/examples/default-ml-ai-tutor\n",
    "* https://beta.openai.com/docs/guides/fine-tuning\n",
    "* https://betterprogramming.pub/how-to-finetune-gpt-3-finetuning-our-virtual-mental-health-assistant-641c1f3b1ef3\n",
    "* https://harishgarg.com/writing/how-to-fine-tune-gpt-3-api/\n",
    "* https://www.kaggle.com/datasets/kouroshalizadeh/history-of-philosophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"ML Tutor: I am a ML/AI language model tutor\\nYou: What is a language model?\\nML Tutor: A language model is a statistical model that describes the probability of a word given the previous words.\\nYou: What is a statistical model?\",\n",
    "  temperature=0.3,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"You:\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f6078",
   "metadata": {},
   "source": [
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05637c8",
   "metadata": {},
   "source": [
    "## 8.) Bug/Code Explainer and Fixer\n",
    "\n",
    "**GOAL: Write an OpenAI based service that explains and automatically creates a docstring for python functions. Could also possibly be an automatic documentation creator (given a repo of code, create documentation website for it on github.io)**\n",
    "\n",
    "* Process:\n",
    "    * Lots of different options here, main differentiator from projects above is that you are now using Codex and returning actual code in some way, not just normal text\n",
    "    \n",
    "* https://beta.openai.com/examples/default-explain-code\n",
    "* https://beta.openai.com/examples/default-fix-python-bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26657aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"code-davinci-002\",\n",
    "  prompt=\"class Log:\\n    def __init__(self, path):\\n        dirname = os.path.dirname(path)\\n        os.makedirs(dirname, exist_ok=True)\\n        f = open(path, \\\"a+\\\")\\n\\n        # Check that the file is newline-terminated\\n        size = os.path.getsize(path)\\n        if size > 0:\\n            f.seek(size - 1)\\n            end = f.read(1)\\n            if end != \\\"\\\\n\\\":\\n                f.write(\\\"\\\\n\\\")\\n        self.f = f\\n        self.path = path\\n\\n    def log(self, event):\\n        event[\\\"_event_id\\\"] = str(uuid.uuid4())\\n        json.dump(event, self.f)\\n        self.f.write(\\\"\\\\n\\\")\\n\\n    def state(self):\\n        state = {\\\"complete\\\": set(), \\\"last\\\": None}\\n        for line in open(self.path):\\n            event = json.loads(line)\\n            if event[\\\"type\\\"] == \\\"submit\\\" and event[\\\"success\\\"]:\\n                state[\\\"complete\\\"].add(event[\\\"id\\\"])\\n                state[\\\"last\\\"] = event\\n        return state\\n\\n\\\"\\\"\\\"\\nHere's what the above class is doing:\\n1.\",\n",
    "  temperature=0,\n",
    "  max_tokens=64,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\\"\\\"\\\"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a29d2f",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b256d6c",
   "metadata": {},
   "source": [
    "## 9.) Document similarity via Text Embedding API\n",
    "\n",
    "* Given a large group of text documents, use Text Embedding API to embed the text docs into vectors, then cluster those vectors via Scikit-Learn.\n",
    "\n",
    "* Process:\n",
    "    * Go to Kaggle and find a nice corpus of text to embed (if you get desperate you can use the classic news article dataset from scikit learn below).\n",
    "    * Embed it via OpenAI Embed API\n",
    "    * Cluster\n",
    "    * Return cluster groups in an easy to understand way (often students get confused on this part, I've seen them want to know \"What does this cluster represent?\" Is it possible to have open ai API actually give the cluster names, like \"Sports\" vs \"Politics\" articles\n",
    "    \n",
    "* https://beta.openai.com/docs/guides/embeddings\n",
    "* https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html\n",
    "* https://beta.openai.com/docs/guides/embeddings\n",
    "\n",
    "\n",
    "**FEEL FREE TO CHOOSE A DIFFERENT USE CASE, LOTS OF EXAMPLES HERE: https://beta.openai.com/docs/guides/embeddings/use-cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbf1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Embedding.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "embeddings = response['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3378567",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab42f0",
   "metadata": {},
   "source": [
    "## 10.) Interview Prep Bot\n",
    "\n",
    "Nervous about an upcoming job interview, this webservice will automatically interview and grade your replies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2b8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f863d4",
   "metadata": {},
   "source": [
    "## 11.) Foreign Newspaper summary\n",
    "\n",
    "**Goal: Give a summary of a foreign newspaper. Pick a German Newspaper, translate front page to english, then summarize that translation to a few bullet points. THat way a simple user can ask \"What is going on in {Germany}?\" and the country input then picks a country newspaper, translates it, and summarizes it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d493c70",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Other Ideas if some of the ideas above are too similar:\n",
    "\n",
    "* Automatic children's night time story teller\n",
    "* Automatic Email reply, fine-tuned on your own emails\n",
    "* Diaraization Transcript meeting summarizer\n",
    "* Summary of earnings calls, listen to a public company earnings calls and give a summary\n",
    "* Summary of SEC fillings froma company, read a 10-K and give an easy to read summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7323061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
